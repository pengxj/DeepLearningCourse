{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习基础\n",
    "### 从零开始写K均值聚类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #本机torch和matplotlib使用的一个dll冲突，先导入torch\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "print(y)\n",
    "plt.scatter(x=X[y==0,0], y=X[y==0,1],c='r')\n",
    "plt.scatter(x=X[y==1,0], y=X[y==1,1],c='g')\n",
    "plt.scatter(x=X[y==2,0], y=X[y==2,1],c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "class KMeansClusterer:\n",
    "    def __init__(self,ndarray,cluster_num):\n",
    "        self.ndarray = ndarray\n",
    "        self.cluster_num = cluster_num\n",
    "        self.points=self.__pick_start_point(ndarray,cluster_num)\n",
    "         \n",
    "    def cluster(self):\n",
    "        result = []\n",
    "        for i in range(self.cluster_num):\n",
    "            result.append([])\n",
    "        for item in self.ndarray:\n",
    "            distance_min = sys.maxsize\n",
    "            index=-1\n",
    "            for i in range(len(self.points)):                \n",
    "                distance = self.__distance(item,self.points[i])\n",
    "                if distance < distance_min:\n",
    "                    distance_min = distance\n",
    "                    index = i\n",
    "            result[index] = result[index] + [item.tolist()]\n",
    "        new_center=[]\n",
    "        for item in result:\n",
    "            new_center.append(self.__center(item).tolist())\n",
    "        # 中心点未改变，说明达到稳态，结束递归\n",
    "        if (self.points==new_center).all():\n",
    "            return result\n",
    "         \n",
    "        self.points=np.array(new_center)\n",
    "        return self.cluster()\n",
    "             \n",
    "    def __center(self,list):\n",
    "        '''计算一组坐标的中心点\n",
    "        '''\n",
    "        # 计算每一列的平均值\n",
    "        return np.array(list).mean(axis=0)\n",
    "    def __distance(self,p1,p2):\n",
    "        '''计算两点间距\n",
    "        '''\n",
    "        tmp=0\n",
    "        for i in range(len(p1)):\n",
    "            tmp += pow(p1[i]-p2[i],2)\n",
    "        return pow(tmp,0.5)\n",
    "    def __pick_start_point(self,ndarray,cluster_num):\n",
    "        \n",
    "        if cluster_num <0 or cluster_num > ndarray.shape[0]:\n",
    "            raise Exception(\"K设置有误\")\n",
    "      \n",
    "        # 随机点的下标\n",
    "        indexes=random.sample(np.arange(0,ndarray.shape[0],step=1).tolist(),cluster_num)\n",
    "        points=[]\n",
    "        for index in indexes:\n",
    "            points.append(ndarray[index].tolist())\n",
    "        return np.array(points)\n",
    "kmeans = KMeansClusterer(X[:,:2], 3)\n",
    "clusters = kmeans.cluster()\n",
    "plt.scatter(x=np.array(clusters[0])[:,0], y=np.array(clusters[0])[:,1],c='r')\n",
    "plt.scatter(x=np.array(clusters[1])[:,0], y=np.array(clusters[1])[:,1],c='g')\n",
    "plt.scatter(x=np.array(clusters[2])[:,0], y=np.array(clusters[2])[:,1],c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用sklearn进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "plt.scatter(x=X[kmeans.labels_==0,0], y=X[kmeans.labels_==0,1],c='r')\n",
    "plt.scatter(x=X[kmeans.labels_==1,0], y=X[kmeans.labels_==1,1],c='g')\n",
    "plt.scatter(x=X[kmeans.labels_==2,0], y=X[kmeans.labels_==2,1],c='b')\n",
    "plt.scatter(x=kmeans.cluster_centers_[:,0], y=kmeans.cluster_centers_[:,1], marker='+',c='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始写线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# y = -1.2 * x ** 2 + 1.1 * x + 1\n",
    "# white_noise = np.random.standard_normal(size=50)\n",
    "# y += 0.05*white_noise\n",
    "x = np.linspace(0, 1, 50)\n",
    "y = np.array([1.07238178,1.0170364,0.98098772,1.00370863,1.04445061,1.10602251\n",
    ",1.10313956,1.15632729,1.15341822,1.10110894,1.20492149,1.20728225\n",
    ",1.18456962,1.179104,1.1661111,1.29627646,1.15055874,1.2338195\n",
    ",1.20024153,1.28895788,1.2971685,1.1562486,1.22715693,1.2563372\n",
    ",1.29407935,1.24687405,1.34805009,1.18117945,1.27979809,1.22611831\n",
    ",1.21083584,1.22060672,1.16847706,1.19524818,1.18637369,1.16826647\n",
    ",1.11734901,1.13540003,1.1701965,1.11857396,1.12874242,1.09255254\n",
    ",1.11020603,1.00728039,1.03384228,0.97602499,1.00627464,0.92149258\n",
    ",0.95858817,0.89845713])#创建数组\n",
    "\n",
    "alpha= 0.1 #步长\n",
    "# initial_w = 0\n",
    "initial_w = [0,0]\n",
    "initial_b = 0\n",
    "num_iter =10000 #迭代次数\n",
    "\n",
    "# xx = x\n",
    "x_hat = np.concatenate((x**2, x),axis=0)\n",
    "xx = x_hat.reshape((2,50))\n",
    "xx = xx.T\n",
    "\n",
    "def compute_cost(w,b):\n",
    "    total_cost=0\n",
    "    M =50\n",
    "    for i in range(M):\n",
    "        total_cost += (np.dot(w,xx[i]) + b - y[i])**2\n",
    "    return total_cost/M\n",
    "\n",
    "def step_grad_desc(current_w, current_b, alpha):\n",
    "    sum_grad_w = initial_w\n",
    "    sum_grad_b = 0\n",
    "    M = 50\n",
    "    for i in range(M):\n",
    "        sum_grad_w += (np.dot(current_w, xx[i]) + current_b - y[i]) * xx[i]\n",
    "        sum_grad_b += np.dot(current_w, xx[i]) + current_b - y[i]\n",
    "    # 用公式求当前梯度\n",
    "    grad_w = 2 / M * sum_grad_w\n",
    "    grad_b = 2 / M * sum_grad_b\n",
    "\n",
    "    # 梯度下降，更新当前的w和b\n",
    "    updated_w = current_w - alpha * grad_w\n",
    "    updated_b = current_b - alpha * grad_b\n",
    "\n",
    "    return updated_w, updated_b\n",
    "\n",
    "def grad_desc( initial_w, initial_b, alpha, num_iter):\n",
    "    w = initial_w\n",
    "    b = initial_b\n",
    "    # 定义一个list保存所有的损失函数值，用来显示下降过程。\n",
    "    cost_list = []\n",
    "    for i in range(num_iter):\n",
    "        cost_list.append(compute_cost(w, b))\n",
    "        w, b = step_grad_desc(w, b, alpha)\n",
    "\n",
    "    return [w, b, cost_list]\n",
    "\n",
    "w,b,cost_list= grad_desc(initial_w,initial_b,alpha,num_iter)\n",
    "print (\"w is :\",w)\n",
    "print (\"b is :\",b)\n",
    "plt.figure()\n",
    "plt.title(\"cost\")\n",
    "plt.plot(range(num_iter),cost_list[:])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y)\n",
    "pred_y= np.dot(xx, w) + b\n",
    "plt.plot(x,pred_y,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用sklearn进行线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(xx, y)\n",
    "print(regr.coef_)\n",
    "plt.scatter(x,y)\n",
    "pred_y = regr.predict(xx)\n",
    "plt.plot(x,pred_y,c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch前向神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 15)\n",
    "        self.fc2 = nn.Linear(15, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "t = torch.randn(1,784)\n",
    "mlp = MLP()\n",
    "mlp(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP2 = nn.Sequential(nn.Linear(784, 15), nn.Linear(15, 10))\n",
    "t = torch.randn(1,784)\n",
    "MLP2(t)\n",
    "#------另一种写法-----\n",
    "net = nn.Sequential()\n",
    "net.add_module('fc1', nn.Linear(784,15))\n",
    "net.add_module('fc2', nn.Linear(15,10))\n",
    "net\n",
    "#------另外一种写法-------\n",
    "from collections import OrderedDict\n",
    "net2 = nn.Sequential(OrderedDict([\n",
    " ('fc1', nn.Linear(784,15)),\n",
    "    ('fc2', nn.Linear(15,10))\n",
    "]))\n",
    "net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数的访问和初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in mlp.parameters(): #查看模型所有的可学习参数，此函数将返回⼀个⽣成器。\n",
    "    print(param.shape, param.dtype)\n",
    "    \n",
    "mlp.fc2.weight #指定某层参数 或者如下\n",
    "list(mlp.parameters())[2]\n",
    "#------初始化参数-------\n",
    "from torch.nn import init\n",
    "init.normal_(mlp.fc2.weight, mean=0, std=0.01)\n",
    "mlp.fc2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch的自动求导\n",
    "f(x) = x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3,3,requires_grad=True)\n",
    "print(t1)\n",
    "t2 = t1.pow(2).sum() #平方和 标量\n",
    "t2.backward()\n",
    "t1.grad\n",
    "# t1.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP手写数字识别训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.utils.data.dataloader as Data\n",
    "\n",
    "def one_hot(label, depth=10):\n",
    "    out = torch.zeros(label.size(0), depth)\n",
    "    idx = torch.LongTensor(label).view(-1, 1)\n",
    "    out.scatter_(dim=1, index=idx, value=1)\n",
    "    return out\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    './data', train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    './data', train=False, transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "print(\"train_data:\", train_data.train_data.size())\n",
    "print(\"train_labels:\", train_data.train_labels.size())\n",
    "print(\"test_data:\", test_data.test_data.size())\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=64)\n",
    "# MLP 模型\n",
    "model = nn.Sequential(nn.Linear(784, 15),nn.Sigmoid(), nn.Linear(15, 10))\n",
    "print(model)\n",
    "model.cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "# loss_func = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    # training-----------------------------\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.view(-1, 784)\n",
    "        batch_y_onehot = one_hot(batch_y,10)\n",
    "        batch_x, batch_y_onehot, batch_y = batch_x.cuda(), batch_y_onehot.cuda(), batch_y.cuda()\n",
    "        out = model(batch_x)\n",
    "#         loss = loss_func(out, batch_y_onehot)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        train_loss += loss.cpu().item()\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        train_correct = (pred == batch_y).sum()\n",
    "        train_acc += train_correct.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "        train_data)), train_acc / (len(train_data))))\n",
    "\n",
    "    # evaluation--------------------------------\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x = batch_x.view(-1, 784)\n",
    "            batch_y_onehot = one_hot(batch_y,10)\n",
    "            batch_x, batch_y_onehot, batch_y = batch_x.cuda(), batch_y_onehot.cuda(), batch_y.cuda()\n",
    "            out = model(batch_x)\n",
    "#             loss = loss_func(out, batch_y_onehot)\n",
    "            loss = loss_func(out, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct.item()\n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "            test_data)), eval_acc / (len(test_data))))\n",
    "    torch.save(model.state_dict(), f'mnist_mlp_epoch-{epoch}.pt')\n",
    "#-----加载测试-------\n",
    "model = nn.Sequential(nn.Linear(784, 15),nn.Sigmoid(), nn.Linear(15, 10))\n",
    "model.load_state_dict(torch.load('mnist_mlp_epoch-1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
