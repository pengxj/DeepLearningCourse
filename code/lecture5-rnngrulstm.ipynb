{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    "### 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4502, 0.8332, 0.0000, 2.4592]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "X, W_xh = torch.randn(1, 5), torch.randn(5, 4) #X为5维数据，时序为1\n",
    "H, W_hh = torch.randn(1, 4), torch.randn(4, 4)\n",
    "b = torch.ones(1,1)\n",
    "h_t = F.relu(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b)\n",
    "h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch模块定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0656,  0.1010, -0.1321,  0.2298, -0.0287, -0.2559, -0.3701, -0.3275,\n",
       "         -0.5198,  0.5142]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RnnNet(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        super(RnnNet, self).__init__()\n",
    "        self.fc_x2h = nn.Linear(dim_input, dim_hidden)\n",
    "        self.fc_h2h = nn.Linear(dim_hidden, dim_hidden, bias = False)\n",
    "        self.fc_h2y = nn.Linear(dim_hidden, dim_output)  #4x1\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x.new_zeros(1, self.dim_hidden)\n",
    "        for t in range(x.size(0)):\n",
    "            h = F.relu(self.fc_x2h(x[t:t+1]) + self.fc_h2h(h)) #\n",
    "        return self.fc_h2y(h)\n",
    "\n",
    "rnn = RnnNet(5, 20, 10)\n",
    "t = torch.randn(20, 5) #时序长为20\n",
    "rnn(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 2, batch_first=True) # inputsize, hidden size, num_layers\n",
    "input = torch.randn(3, 5, 10) # batchsize 3  时序长度为5， 10：特征维度\n",
    "h0 = torch.randn(2, 3, 20) # 层数，batchsize, hiddensize\n",
    "output, hn = rnn(input, h0)  #\n",
    "print(hn.shape)\n",
    "output.shape  # W_hy x H  输出size默认为h的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  使用RNNCell进行单个样本运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNNCell(10, 20)\n",
    "input = torch.randn(6, 3, 10) # (time_steps, batch, input_size)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(input.size(0)):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)\n",
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gating RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1977,  0.0974,  0.0241, -0.1042]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RecNetWithGating(nn.Module):\n",
    "    def __init__(self, dim_input, dim_recurrent, dim_output):\n",
    "        super(RecNetWithGating, self).__init__()\n",
    "        self.fc_x2h = nn.Linear(dim_input, dim_recurrent)\n",
    "        self.fc_h2h = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
    "        self.fc_x2z = nn.Linear(dim_input, dim_recurrent)\n",
    "        self.fc_h2z = nn.Linear(dim_recurrent, dim_recurrent, bias = False)\n",
    "        self.fc_h2y = nn.Linear(dim_recurrent, dim_output)\n",
    "        self.dim_hidden = dim_recurrent\n",
    "    def forward(self, input):\n",
    "        h = input.new_zeros(1, self.dim_hidden)\n",
    "        for t in range(input.size(0)):\n",
    "            z = torch.sigmoid(self.fc_x2z(input[t:t+1]) + self.fc_h2z(h))\n",
    "            hb = F.relu(self.fc_x2h(input[t:t+1]) + self.fc_h2h(h))\n",
    "            h = z * h + (1 - z) * hb\n",
    "        return self.fc_h2y(h)\n",
    "    \n",
    "rnn = RecNetWithGating(5, 4, 4)\n",
    "t = torch.randn(20, 5) #时序长为20\n",
    "rnn(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTMCell(10, 20) #input_dim, recurrent dim\n",
    "input = torch.randn(2, 3, 10) # (time_steps, batch, input_size)\n",
    "hx = torch.randn(3, 20) # (batch, hidden_size)\n",
    "cx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(input.size()[0]):\n",
    "    hx, cx = lstm(input[i], (hx, cx)) # 每次输入一个时间样本\n",
    "    output.append(hx)\n",
    "output = torch.stack(output, dim=0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class lstmNet(nn.Module):\n",
    "    def __init__(self, dim_input, dim_recurrent, num_layers, dim_output):\n",
    "        super(lstmNet, self).__init__()\n",
    "        self.lstm = nn.LSTM(dim_input, dim_recurrent, num_layers)\n",
    "        self.fc = nn.Linear(dim_recurrent, dim_output)\n",
    "    def forward(self, x):\n",
    "        hx, cx = self.lstm(x)\n",
    "        o = hx[-1,:,:]\n",
    "        o = o.squeeze(axis=0)\n",
    "        return self.fc(o)\n",
    "\n",
    "input = torch.randn(2, 3, 10) #T N C\n",
    "lstm = lstmNet(10, 20, 1, 7)\n",
    "output = lstm(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class gruNet(nn.Module):\n",
    "    def __init__(self, dim_input, dim_recurrent, num_layers, dim_output):\n",
    "        super(gruNet, self).__init__()\n",
    "        self.gru = nn.GRU(dim_input, dim_recurrent, num_layers)\n",
    "        self.fc = nn.Linear(dim_recurrent, dim_output)\n",
    "    def forward(self, x):\n",
    "        hx, cx = self.gru(x)\n",
    "        o = hx[-1,:,:]\n",
    "        o = o.squeeze(axis=0)\n",
    "        return self.fc(o)\n",
    "\n",
    "input = torch.randn(2, 3, 10) #T N C\n",
    "gru = gruNet(10, 20, 1, 7)\n",
    "output = gru(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB 文本情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model/ws.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d3fa0f029bbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model/ws.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/ws.pkl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import re\n",
    " \n",
    "# 路径需要根据情况修改，文件太大的时候可以引用绝对路径\n",
    "data_base_path = r\"F:\\SZTU-教学文件\\UG-深度学习方法与应用\\examples\\data\\aclImdb_v1\\aclImdb\"\n",
    " \n",
    "#1. 定义tokenize的方法，对评论文本分词\n",
    "def tokenize(text):\n",
    "    # fileters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    fileters = ['!','\"','#','$','%','&','\\(','\\)','\\*','\\+',',','-','\\.','/',':',';','<','=','>','\\?','@'\n",
    "        ,'\\[','\\\\','\\]','^','_','`','\\{','\\|','\\}','~','\\t','\\n','\\x97','\\x96','”','“',]\n",
    "    # sub方法是替换\n",
    "    text = re.sub(\"<.*?>\",\" \",text,flags=re.S)# 去掉<...>中间的内容，主要是文本内容中存在<br/>等内容\n",
    "    text = re.sub(\"|\".join(fileters),\" \",text,flags=re.S)# 替换掉特殊字符，'|'是把所有要匹配的特殊字符连在一起\n",
    "    return [i.strip() for i in text.split()]# 去掉前后多余的空格\n",
    "\n",
    "\n",
    "#2. 准备dataset\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self,mode):\n",
    "        super(ImdbDataset,self).__init__()\n",
    "        # 读取所有的训练文件夹名称\n",
    "        if mode==\"train\":\n",
    "            text_path = [os.path.join(data_base_path,i)  for i in [\"train/neg\",\"train/pos\"]]\n",
    "        else:\n",
    "            text_path =  [os.path.join(data_base_path,i)  for i in [\"test/neg\",\"test/pos\"]]\n",
    " \n",
    "        self.total_file_path_list = []\n",
    "        # 进一步获取所有文件的名称\n",
    "        for i in text_path:\n",
    "            self.total_file_path_list.extend([os.path.join(i,j) for j in os.listdir(i)])\n",
    " \n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        cur_path = self.total_file_path_list[idx]\n",
    "        cur_filename = os.path.basename(cur_path)\n",
    "        # 标题的形式是：3_4.txt\t前面的3是索引，后面的4是分类\n",
    "        # 原本的分类是1-10，现在变为0-9\n",
    "        label = int(cur_filename.split(\"_\")[-1].split(\".\")[0]) -1 #处理标题，获取label，-1是因为要转化为[0-9]\n",
    "        text = tokenize(open(cur_path).read().strip()) #直接按照空格进行分词\n",
    "        return label,text\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.total_file_path_list)\n",
    "    \n",
    "# 测试是否能成功获取数据\n",
    "dataset = ImdbDataset(mode=\"train\")\n",
    "print(dataset[0])\n",
    "\n",
    "# Word2Sequence\n",
    "class Word2Sequence:\n",
    "    # 未出现过的词\n",
    "    UNK_TAG = \"UNK\"\n",
    "    PAD_TAG = \"PAD\"\n",
    "    # 填充的词\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    " \n",
    "    def __init__(self):\n",
    "        self.dict = {\n",
    "            self.UNK_TAG: self.UNK,\n",
    "            self.PAD_TAG: self.PAD\n",
    "        }\n",
    "        self.count = {}\n",
    " \n",
    "    def to_index(self, word):\n",
    "        \"\"\"word -> index\"\"\"\n",
    "        return self.dict.get(word, self.UNK)\n",
    " \n",
    "    def to_word(self, index):\n",
    "        \"\"\"index -> word\"\"\"\n",
    "        if index in self.inversed_dict:\n",
    "            return self.inversed_dict[index]\n",
    "        return self.UNK_TAG\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    " \n",
    "    def fit(self, sentence):\n",
    "        \"\"\"count字典中存储每个单词出现的次数\"\"\"\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    " \n",
    "    def build_vocab(self, min_count=None, max_count=None, max_feature=None):\n",
    "        \"\"\"\n",
    "        构建词典\n",
    "        只筛选出现次数在[min_count,max_count]之间的词\n",
    "        词典最大的容纳的词为max_feature，按照出现次数降序排序，要是max_feature有规定，出现频率很低的词就被舍弃了\n",
    "        \"\"\"\n",
    "        if min_count is not None:\n",
    "            self.count = {word: count for word, count in self.count.items() if count >= min_count}\n",
    " \n",
    "        if max_count is not None:\n",
    "            self.count = {word: count for word, count in self.count.items() if count <= max_count}\n",
    " \n",
    "        if max_feature is not None:\n",
    "            self.count = dict(sorted(self.count.items(), lambda x: x[-1], reverse=True)[:max_feature])\n",
    "        # 给词典中每个词分配一个数字ID\n",
    "        for word in self.count:\n",
    "            self.dict[word] = len(self.dict)\n",
    "        # 构建一个数字映射到单词的词典，方法反向转换，但程序中用不太到\n",
    "        self.inversed_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
    " \n",
    "    def transform(self, sentence, max_len=None):\n",
    "        \"\"\"\n",
    "        根据词典给每个词分配的数字ID，将给定的sentence（字符串序列）转换为数字序列\n",
    "        max_len：统一文本的单词个数\n",
    "        \"\"\"\n",
    "        if max_len is not None:\n",
    "            r = [self.PAD] * max_len\n",
    "        else:\n",
    "            r = [self.PAD] * len(sentence)\n",
    "        # 截断文本\n",
    "        if max_len is not None and len(sentence) > max_len:\n",
    "            sentence = sentence[:max_len]\n",
    "        for index, word in enumerate(sentence):\n",
    "            r[index] = self.to_index(word)\n",
    "        return np.array(r, dtype=np.int64)\n",
    " \n",
    "    def inverse_transform(self, indices):\n",
    "        \"\"\"数字序列-->单词序列\"\"\"\n",
    "        sentence = []\n",
    "        for i in indices:\n",
    "            word = self.to_word(i)\n",
    "            sentence.append(word)\n",
    "        return sentence\n",
    "\n",
    "# 自定义的collate_fn方法\n",
    "def collate_fn(batch):\n",
    "    # 手动zip操作，并转换为list，否则无法获取文本和标签了\n",
    "    batch = list(zip(*batch))\n",
    "    labels = torch.tensor(batch[0], dtype=torch.int32)\n",
    "    texts = batch[1]\n",
    "    texts = torch.tensor([ws.transform(i, max_len) for i in texts])\n",
    "    del batch\n",
    "    # 注意这里long()不可少，否则会报错\n",
    "    return labels.long(), texts.long()\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 500\n",
    "max_len = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_dataloader(train=True):\n",
    "    if train:\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        mode = \"test\"\n",
    "    dataset = ImdbDataset(mode)\n",
    "    batch_size = train_batch_size if train else test_batch_size\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "dataloader = get_dataloader()\n",
    "\n",
    "for idx,(label,text) in enumerate(dataloader):\n",
    "    print(\"idx：\",idx)\n",
    "    print(\"label:\",label)\n",
    "    print(\"text:\",text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4a972787098f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./model/ws.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# 建立词表\n",
    "def fit_save_word_sequence():\n",
    "    word_to_sequence = Word2Sequence()\n",
    "    train_path = [os.path.join(data_base_path, i) for i in [\"train/neg\", \"train/pos\"]]\n",
    "    # total_file_path_list存储总的需要读取的txt文件\n",
    "    total_file_path_list = []\n",
    "    for i in train_path:\n",
    "        total_file_path_list.extend([os.path.join(i, j) for j in os.listdir(i)])\n",
    "    # tqdm是显示进度条的\n",
    "    for cur_path in tqdm(total_file_path_list, ascii=True, desc=\"fitting\"):\n",
    "        word_to_sequence.fit(tokenize(open(cur_path, encoding=\"utf-8\").read().strip()))\n",
    "    word_to_sequence.build_vocab()\n",
    "    # 对wordSequesnce进行保存\n",
    "    pickle.dump(word_to_sequence, open(\"model/ws.pkl\", \"wb\"))\n",
    "\n",
    "    \n",
    "ws = pickle.load(open(\"./model/ws.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
